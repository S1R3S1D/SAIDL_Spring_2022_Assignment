{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN_with_SimCLR.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOtUI4PYbn8m+SX0YLt/wnT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/S1R3S1D/SAIDL_Spring_2022_Assignment/blob/main/Reinforcement_Learning/DQN_with_SimCLR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Runtime Dependencies\n",
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ],
      "metadata": {
        "id": "8eRUjg2TV7h4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TtBQ5YsOUejn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import gym\n",
        "import os\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from collections import deque\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Projection Head\n",
        "\n",
        "class projection_head(nn.Module):\n",
        "    def __init__(self, embed_dim=1024, output_dim=128):\n",
        "\n",
        "        super(projection_head, self).__init__()\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(self.embed_dim, 2048),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(2048, self.output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.projection(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "x99mGv9uUoc9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model\n",
        "\n",
        "class ConvNN(nn.Module):\n",
        "  def __init__(self, emb_dim):\n",
        "\n",
        "    super(ConvNN, self).__init__()\n",
        "\n",
        "    self.conv_net = nn.Sequential(\n",
        "\n",
        "        nn.Conv2d(4, 32, 8, 4),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Conv2d(32, 64, 4, 2),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Conv2d(64, 64,3, 1),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    self.flatten = nn.Flatten(start_dim = 1)\n",
        "\n",
        "    self.fc_layer = nn.Sequential(\n",
        "        nn.Linear(64*7*7, 512),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(512, emb_dim),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = self.conv_net(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.fc_layer(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "1KnVbEYCUz4s"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Wrappers\n",
        "\n",
        "class cropped84x84_grayscale(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(cropped84x84_grayscale, self).__init__(env)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        observation = Image.fromarray(observation)\n",
        "\n",
        "        observation = observation.crop((0, 35, 160, 190))\n",
        "\n",
        "        observation = observation.resize((84, 84), Image.LANCZOS)\n",
        "\n",
        "        observation = observation.convert('L')\n",
        "\n",
        "        observation = np.asarray(observation)\n",
        "\n",
        "        return observation\n",
        "\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env, skip = 4):\n",
        "        super(MaxAndSkipEnv, self).__init__(env)\n",
        "        self._obs_buffer = deque(maxlen=2)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for _ in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            self._obs_buffer.append(obs)\n",
        "            total_reward+=reward\n",
        "            if done:\n",
        "                break\n",
        "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
        "        return max_frame, total_reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        self._obs_buffer.clear()\n",
        "        obs = self.env.reset()\n",
        "        self._obs_buffer.append(obs)\n",
        "        return obs\n"
      ],
      "metadata": {
        "id": "sSHJA71jVDVb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset\n",
        "\n",
        "class OpenAIGymData(Dataset):\n",
        "  def __init__(self, env, fire = False, n_obs = 2560):\n",
        "\n",
        "    os.mkdir('Data')\n",
        "    os.mkdir('Data/AtariEnvImageData')\n",
        "\n",
        "    self.n_obs = n_obs\n",
        "    # env = gym.wrappers.AtariPreprocessing(env, noop_max=30, screen_size=84, terminal_on_life_loss=True, grayscale_obs=True)\n",
        "    env = cropped84x84_grayscale(env)\n",
        "    env = MaxAndSkipEnv(env)\n",
        "    env = gym.wrappers.FrameStack(env, 4)\n",
        "    env.reset()\n",
        "    if fire:\n",
        "      env.step(1)\n",
        "    data_folder = ''\n",
        "    self.data_folder = data_folder\n",
        "    for i in range(int(16)):\n",
        "      file_name = data_folder+'/'+str(i)+'.npy'\n",
        "      with open(file_name, 'wb') as f:\n",
        "        samples = []\n",
        "        for i in range(int(self.n_obs/16)):\n",
        "          action = np.random.randint(0, env.action_space.n)\n",
        "          (sample, reward, done, info) = env.step(action)\n",
        "          if done == True:\n",
        "            env.reset()\n",
        "            if fire:\n",
        "              env.step(1)\n",
        "\n",
        "          sample = np.array(sample)\n",
        "          sample = sample.astype(float)/255.0\n",
        "          samples.append(sample)\n",
        "        samples = np.array(samples)\n",
        "        np.save(f, samples)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_obs\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    file_no = str(int(index/int(self.n_obs/16)))\n",
        "    in_file_index = index%int(self.n_obs/16)\n",
        "\n",
        "    file_name = self.data_folder+'/'+file_no+'.npy'\n",
        "    with open(file_name, 'rb') as f:\n",
        "      sample = np.load(f)\n",
        "      return sample[in_file_index]"
      ],
      "metadata": {
        "id": "aaMLEMw4U1pj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data\n",
        "!rm -rf Data/\n",
        "env = gym.make('BreakoutNoFrameskip-v4')\n",
        "breakout_dataset = OpenAIGymData(env)\n"
      ],
      "metadata": {
        "id": "41gTO9lBVfP_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DataLoader\n",
        "breakout_dataloader = DataLoader(breakout_dataset, batch_size=32)"
      ],
      "metadata": {
        "id": "aeg_pI2CZ6-E"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loss Function\n",
        "\n",
        "class ntxent(nn.Module):\n",
        "    def __init__(self, batch_size, temperature):\n",
        "        super(ntxent, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.register_buffer(\"neg_eye\", (~torch.eye(batch_size * 2, batch_size * 2, dtype=bool)).float())\n",
        "        self.register_buffer(\"temperature\", torch.tensor(temperature))\n",
        "    def forward(self, emb_i, emb_j):\n",
        "\n",
        "        z_i = F.normalize(emb_i)\n",
        "        z_j = F.normalize(emb_j)\n",
        "\n",
        "        z = torch.cat([z_i, z_j], 0)\n",
        "\n",
        "        sim_mat = F.cosine_similarity(z.unsqueeze(1), z.unsqueeze(0), dim=2)\n",
        "        # print(\"shape of similarity matrix :\", sim_mat.shape)\n",
        "\n",
        "        sim_ij = torch.diag(sim_mat, self.batch_size)\n",
        "        sim_ji = torch.diag(sim_mat, -self.batch_size)\n",
        "\n",
        "        positives = torch.cat([sim_ij, sim_ji], 0)\n",
        "\n",
        "        numerator = torch.exp(positives/self.temperature)\n",
        "        # print(\"Shape of positives:\", positives.shape)\n",
        "        denominator = self.neg_eye*torch.exp(sim_mat/self.temperature)\n",
        "\n",
        "        loss_partial = -torch.log(numerator/(torch.sum(denominator, 0)-numerator))\n",
        "\n",
        "        loss = torch.sum(loss_partial)/(2*self.batch_size)\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "NitNT_stWVvy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameters and rest\n",
        "\n",
        "lr = 0.001\n",
        "\n",
        "model = ConvNN(1024)\n",
        "projection = projection_head(1024, 128)\n",
        "\n",
        "params = list(model.parameters())+list(projection.parameters())\n",
        "\n",
        "optimizer = torch.optim.Adam(params, lr=lr)\n",
        "\n",
        "loss_fn = ntxent(32, 0.07)\n",
        "\n",
        "epochs = 2"
      ],
      "metadata": {
        "id": "ikAraYy0Viw1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training Model for contrastive learning\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  for images in breakout_dataloader:\n",
        "    images = images.clone().detach().float()\n",
        "\n",
        "    embed1 = model(images)\n",
        "    proj1 = projection(embed1)\n",
        "\n",
        "    embed2 = model(images)\n",
        "    proj2 = projection(embed2)\n",
        "\n",
        "    loss = loss_fn(proj1, proj2)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "lRfnSqjyWc-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training DQN with model embeddings\n",
        "\n",
        "#Deep Q Network Acrhitecture\n",
        "\n",
        "class DQN(nn.Module):\n",
        "  def __init__(self, n_actions):\n",
        "    \n",
        "    super(DQN, self).__init__()\n",
        "\n",
        "    self.n_actions = n_actions\n",
        "\n",
        "    self.FC = nn.Sequential(\n",
        "        nn.Linear(1024, 2048),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(2048, 512),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(512, self.n_actions)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = self.FC(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "nb8xxkuUW1tq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameters, loss function, optimizer and others for Deep Q learning\n",
        "\n",
        "lr = 0.001\n",
        "\n",
        "dqn = DQN(env.action_space.n)\n",
        "\n",
        "target_network = copy.deepcopy(dqn)\n",
        "target_network.load_state_dict(dqn.state_dict())\n",
        "\n",
        "sync_freq = 5000\n",
        "\n",
        "loss_fn_rl = nn.MSELoss()\n",
        "\n",
        "optimizer_rl = torch.optim.Adam(dqn.parameters(), lr = lr)\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)\n",
        "\n",
        "dqn.to(device)\n",
        "target_network.to(device)"
      ],
      "metadata": {
        "id": "EnK41H5Bps7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DQN testing module\n",
        "\n",
        "def test_dqn(env, render = False):\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    print(\"Testing Dqn:\")\n",
        "    env.reset()\n",
        "    tot_pos_rew = 0\n",
        "    tot_reward = 0\n",
        "    (state, reward, done, info) = env.step(np.random.randint(0, env.action_space.n))\n",
        "    while not done:\n",
        "      state = torch.tensor(np.expand_dims(np.array(state), axis=0)).float()/255.0\n",
        "      state = state.to(device)\n",
        "\n",
        "      qval_ = dqn(state)\n",
        "      qval = qval_.cpu().detach().squeeze().numpy()\n",
        "      \n",
        "      if np.random.random()<0.1:\n",
        "        action = np.random.randint(0, env.action_space.n)\n",
        "      else:\n",
        "        action = np.argmax(qval)\n",
        "      \n",
        "      (state, reward, done, info) = env.step(action)\n",
        "      if render:\n",
        "        plt.imshow(np.array(state)[0])\n",
        "        plt.show()\n",
        "\n",
        "      if reward>0:\n",
        "        tot_pos_rew+=reward\n",
        "\n",
        "      tot_reward+=reward\n",
        "    \n",
        "    print(\"Total Positive Reward :\", tot_pos_rew)\n",
        "    print(\"Total Reward :\", tot_reward)"
      ],
      "metadata": {
        "id": "_vclIHSOqOd8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dqn_w_trgnet_expreplay(n_eps):\n",
        "\n",
        "  replay = deque(maxlen=30000)\n",
        "  mini_batch_size = 32\n",
        "  gamma = 0.99\n",
        "  episode = 0\n",
        "  losses = []\n",
        "  \n",
        "  for _ in range(n_eps):\n",
        "\n",
        "    print(\"Training Episode :\", episode)\n",
        "    epsilon = 1\n",
        "    episode+=1\n",
        "    epoch = 0\n",
        "    env.reset()\n",
        "    (state1, reward1, done1, info1) = env.step(np.random.randint(0, env.action_space.n))\n",
        "\n",
        "    \n",
        "    for _ in tqdm(range(100000)):\n",
        "      \n",
        "      \n",
        "      state1_ = torch.tensor(np.expand_dims(np.array(state1), axis=0)).float()/255.0\n",
        "      state1_ = torch.Tensor(state1_)\n",
        "      state1_ = state1_.to(device)\n",
        "\n",
        "      state1_ = model(state1_)#Model embeddings\n",
        "\n",
        "      qval_ = dqn(state1_)\n",
        "      qval = qval_.cpu().detach().squeeze().numpy()\n",
        "\n",
        "      if random.random()<epsilon:\n",
        "        action = np.random.randint(0, env.action_space.n)\n",
        "      else:\n",
        "        action = np.argmax(qval)\n",
        "\n",
        "      if epsilon>0.1:\n",
        "        epsilon-=1/50000\n",
        "\n",
        "      (state2, reward2, done2, info2) = env.step(action)\n",
        "\n",
        "      if done2:\n",
        "        env.reset()\n",
        "\n",
        "      state2_ = torch.from_numpy(np.expand_dims(np.array(state2), axis=0)).float()/255.0\n",
        "      state2_ = torch.Tensor(state2_)\n",
        "      state2_ = state2_.to(device)\n",
        "\n",
        "      state2_ = model(state2_)#Model embeddings\n",
        "\n",
        "      exp = (state1_, action, state2_, reward2, done2)\n",
        "      replay.append(exp)\n",
        "\n",
        "      state1 = state2\n",
        "\n",
        "      if len(replay)>7000:\n",
        "\n",
        "        mini_batch = random.sample(replay, mini_batch_size)\n",
        "\n",
        "        state1_batch = torch.cat([s1 for (s1, a, s2, r, d) in mini_batch]).to(device)\n",
        "        state2_batch = torch.cat([s2 for (s1, a, s2, r, d) in mini_batch]).to(device)\n",
        "        action_batch = torch.Tensor([a for (s1, a, s2, r, d) in mini_batch]).to(device)\n",
        "        reward_batch = torch.Tensor([r for (s1, a, s2, r, d) in mini_batch]).to(device)\n",
        "        done_batch = torch.Tensor([d for (s1, a, s2, r, d) in mini_batch]).to(device)\n",
        "\n",
        "        Q1 = dqn(state1_batch)\n",
        "        with torch.no_grad():\n",
        "          Q2 = target_network(state2_batch)\n",
        "\n",
        "        Y = reward_batch + gamma * ((1-done_batch)*torch.max(Q2, dim=1)[0])\n",
        "        X = Q1.gather(dim = 1, index = action_batch.long().unsqueeze(dim=1)).squeeze()\n",
        "\n",
        "        loss = loss_fn_rl(X, Y.detach())\n",
        "\n",
        "        optimizer_rl.zero_grad()\n",
        "        loss.backward()\n",
        "        losses.append(loss.item())\n",
        "        optimizer.step()\n",
        "        epoch+=1\n",
        "        if epoch%sync_freq ==0:\n",
        "          target_network.load_state_dict(dqn.state_dict())\n",
        "    if episode%2==0:\n",
        "      test_dqn(env)\n",
        "\n",
        "  losses = np.array(losses)\n",
        "  return losses"
      ],
      "metadata": {
        "id": "GffVpcjAp7sO"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}